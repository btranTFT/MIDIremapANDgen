# -*- coding: utf-8 -*-
"""MusicGen_Training_Wii.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V5XJrdvv0bctieJgME-scMM6U2uX-pb_

# MusicGen Fine-Tuning - Wii Soundfont

Fine-tuning MusicGen on Wii audio dataset.

**Requirements**:
- Google Colab Pro with A100 GPU (recommended)
- High-RAM runtime
- Significant training time (several hours)

## 1. Setup & Installation
"""

# Check GPU (A100 recommended for training)
import torch
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Verify we have enough memory for training
if torch.cuda.is_available():
    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    if mem_gb < 15:
        print("\n  WARNING: Less than 15GB GPU memory. Training may fail.")
        print("   Recommendation: Use A100 GPU or reduce batch size.")

# Install dependencies
print("Installing dependencies...")
print("This may take 3-5 minutes...\n")

# Install system dependencies for PyAV (required by AudioCraft)
print("1/6 Installing system dependencies...")
!apt-get update -qq
!apt-get install -y -qq ffmpeg libavcodec-dev libavformat-dev libavutil-dev libavdevice-dev libavfilter-dev fluidsynth

# Install PyTorch (use latest compatible version for Colab)
print("2/6 Installing PyTorch...")
!pip install -q torch torchaudio

# Install PyAV with pre-built wheel
print("3/6 Installing PyAV (audio/video processing)...")
!pip install -q av

# Install AudioCraft dependencies
print("4/6 Installing AudioCraft dependencies...")
# Core dependencies
!pip install -q xformers einops hydra-core omegaconf num2words scipy transformers
# Audio processing
!pip install -q julius sentencepiece protobuf
# AudioCraft specific dependencies (flashy is the correct package name, not flashy-pytorch)
!pip install -q flashy demucs encodec hydra-colorlog torchdiffeq torchmetrics
# Optional but useful
!pip install -q dora-search 2>/dev/null || echo "dora-search optional, skipping"

# Clone AudioCraft and add to Python path
print("5/6 Setting up AudioCraft from source...")
!rm -rf /content/audiocraft
!git clone -q https://github.com/facebookresearch/audiocraft.git /content/audiocraft

# Add AudioCraft to Python path
import sys
sys.path.insert(0, '/content/audiocraft')

# Install other dependencies
print("6/6 Installing audio processing tools...")
!pip install -q librosa soundfile pretty_midi music21 midi2audio tensorboard wandb

print("\n" + "="*60)
print("INSTALLATION VERIFICATION")
print("="*60)

# Verify installations
import torch
import torchaudio
print(f"\n‚úì PyTorch version: {torch.__version__}")
print(f"‚úì Torchaudio version: {torchaudio.__version__}")
print(f"‚úì CUDA available: {torch.cuda.is_available()}")

try:
    import av
    print(f"‚úì PyAV version: {av.__version__}")
except ImportError:
    print(" PyAV failed to import")

try:
    # Import AudioCraft directly from source
    import audiocraft
    from audiocraft.models import MusicGen
    print(f"‚úì AudioCraft loaded from source")
    print(f"‚úì MusicGen can be imported")

    # Test loading a small model to verify everything works
    print("\n Testing MusicGen model loading...")
    print("   (Downloading model weights - this may take 1-2 minutes...)")
    test_model = MusicGen.get_pretrained('facebook/musicgen-melody')
    print(f"‚úì Successfully loaded MusicGen-small model")
    print(f"   Model sample rate: {test_model.sample_rate} Hz")
    del test_model  # Free memory
    torch.cuda.empty_cache()

    print("\n" + "="*60)
    print("‚úÖ ALL SYSTEMS READY FOR TRAINING!")
    print("="*60)

except Exception as e:
    print(f"\n AudioCraft error: {e}")
    import traceback
    traceback.print_exc()
    print("\n" + "="*60)
    print("  CRITICAL ERROR!")
    print("="*60)
    print("AudioCraft failed to load. Please:")
    print("1. Runtime > Restart runtime")
    print("2. Run this cell again")
    print("="*60)

# Mount Google Drive and setup directories
from google.colab import drive
from pathlib import Path
import json
from datetime import datetime

drive.mount('/content/drive')

PROJECT_DIR = Path('/content/drive/MyDrive/MusicGen_Training')
DATA_DIR = PROJECT_DIR / 'datasets'
OUTPUT_DIR = PROJECT_DIR / 'outputs'
CHECKPOINT_DIR = PROJECT_DIR / 'checkpoints'
LOGS_DIR = PROJECT_DIR / 'logs'

for d in [PROJECT_DIR, DATA_DIR, OUTPUT_DIR, CHECKPOINT_DIR, LOGS_DIR]:
    d.mkdir(exist_ok=True, parents=True)

# Create subdirectories
(DATA_DIR / 'nesmdb').mkdir(exist_ok=True)
(DATA_DIR / 'nesmdb_audio').mkdir(exist_ok=True)
(DATA_DIR / 'fma').mkdir(exist_ok=True)
(DATA_DIR / 'processed').mkdir(exist_ok=True)
(OUTPUT_DIR / 'finetuned_samples').mkdir(exist_ok=True)

print(f"‚úì Project directory: {PROJECT_DIR}")
print(f"‚úì Checkpoints will be saved to: {CHECKPOINT_DIR}")
print(f"‚úì Training logs: {LOGS_DIR}")

"""## 2. Download and Prepare Datasets

"""

# Download or Upload NESMDB dataset
import subprocess
import os
from google.colab import files

WII_DIR = DATA_DIR / 'wii'
WII_ARCHIVE = WII_DIR / "wii_audio.tar.gz"

print("="*60)
print("WII DATASET SETUP")
print("="*60)
print("\nOptions:")
print("  1. Upload your tar.gz file (recommended if you have it)")
print("  2. Use file from Google Drive")
print("  3. Try automatic download")
print()

choice = input("Choose option (1/2/3): ").strip()

if choice == "1":
    # Manual upload
    print("\n Please upload your NESMDB tar.gz file...")
    print("(Look for the file upload dialog)")
    uploaded = files.upload()

    if uploaded:
        # Move uploaded file to correct location
        uploaded_filename = list(uploaded.keys())[0]
        import shutil
        shutil.move(uploaded_filename, str(NESMDB_ARCHIVE))
        print(f"‚úì File uploaded: {NESMDB_ARCHIVE}")
        print(f"  Size: {NESMDB_ARCHIVE.stat().st_size / 1e6:.1f} MB")
    else:
        print("  No file uploaded")

elif choice == "2":
    # From Google Drive
    print("\n Looking for NESMDB file in Google Drive...")
    print("Please enter the path to your tar.gz file in Google Drive")
    print("Example: /content/drive/MyDrive/nesmdb_midi.tar.gz")

    drive_path = input("Path: ").strip()

    if os.path.exists(drive_path):
        import shutil
        shutil.copy(drive_path, str(NESMDB_ARCHIVE))
        print(f"‚úì Copied from Google Drive")
        print(f"  Size: {NESMDB_ARCHIVE.stat().st_size / 1e6:.1f} MB")
    else:
        print(f"  File not found at: {drive_path}")

else:
    # Automatic download
    print("\n‚¨áÔ∏è  Attempting automatic download...")
    NESMDB_URL = "http://deepyeti.ucsd.edu/cdonahue/nesmdb/nesmdb_midi.tar.gz"

    result = subprocess.run(
        ["wget", "-O", str(NESMDB_ARCHIVE), NESMDB_URL],
        capture_output=True,
        text=True
    )

    if result.returncode == 0 and NESMDB_ARCHIVE.exists():
        print(f"‚úì Downloaded successfully")
        print(f"  Size: {NESMDB_ARCHIVE.stat().st_size / 1e6:.1f} MB")
    else:
        print(f"  Download failed: {result.stderr}")

# Extract if archive exists
if NESMDB_ARCHIVE.exists():
    print("\n Extracting archive...")

    extract_result = subprocess.run(
        ["tar", "-xzf", str(NESMDB_ARCHIVE), "-C", str(NESMDB_DIR)],
        capture_output=True,
        text=True
    )

    if extract_result.returncode == 0:
        print("‚úì Extracted successfully")
    else:
        print(f"  Extraction failed: {extract_result.stderr}")
        print("\nTrying alternative extraction...")
        # Try with Python tarfile
        import tarfile
        try:
            with tarfile.open(NESMDB_ARCHIVE, 'r:gz') as tar:
                tar.extractall(path=NESMDB_DIR)
            print("‚úì Extracted with Python tarfile")
        except Exception as e:
            print(f"  Also failed: {e}")

# Find MIDI files
print("\nüîç Searching for MIDI files...")
midi_files = list(NESMDB_DIR.rglob('*.mid')) + list(NESMDB_DIR.rglob('*.midi'))
print(f"Found {len(midi_files)} MIDI files")

if len(midi_files) == 0:
    print("\n‚ö†Ô∏è  No MIDI files found. Checking directory structure...")
    print(f"\nContents of {NESMDB_DIR}:")
    for item in NESMDB_DIR.iterdir():
        if item.is_dir():
            subfiles = list(item.rglob('*.mid'))
            print(f"   {item.name}/ ({len(subfiles)} MIDI files)")
        else:
            print(f"   {item.name} ({item.stat().st_size / 1e6:.1f} MB)")
else:
    print(f"\n‚úì Ready to convert {len(midi_files)} MIDI files to audio")

# Convert MIDI to audio for training
import os
import subprocess
from midi2audio import FluidSynth
from tqdm import tqdm

SOUNDFONT_PATH = "/content/FluidR3_GM.sf2"
SOUNDFONT_URL = "https://member.keymusician.com/Member/FluidR3_GM/FluidR3_GM.sf2"

if not os.path.exists(SOUNDFONT_PATH):
    print("Downloading soundfont...")
    subprocess.run(["wget", "-q", "-O", SOUNDFONT_PATH, SOUNDFONT_URL], check=False)

NESMDB_AUDIO_DIR = DATA_DIR / 'nesmdb_audio'

# Convert ALL MIDI files (not just 100) for training
existing_audio = len(list(NESMDB_AUDIO_DIR.glob('*.wav')))
print(f"Existing audio files: {existing_audio}")

if existing_audio < len(midi_files):
    print(f"Converting {len(midi_files)} MIDI files to audio...")
    print("This will take 10-20 minutes...")

    fs = FluidSynth(SOUNDFONT_PATH)
    converted = 0
    failed = 0

    for midi_file in tqdm(midi_files):
        output_file = NESMDB_AUDIO_DIR / f"{midi_file.stem}.wav"
        if not output_file.exists():
            try:
                fs.midi_to_audio(str(midi_file), str(output_file))
                converted += 1
            except Exception as e:
                failed += 1

    print(f"‚úì Converted {converted} new files")
    if failed > 0:
        print(f"  {failed} files failed to convert")

total_audio = len(list(NESMDB_AUDIO_DIR.glob('*.wav')))
print(f"\n‚úì Total audio files ready for training: {total_audio}")

# Optional: Download FMA Small dataset (7.2 GB)
# Uncomment to include FMA in training

download_fma = input("Download FMA Small dataset (7.2 GB, ~15 min)? (y/n): ").lower() == 'y'

if download_fma:
    FMA_DIR = DATA_DIR / 'fma'
    FMA_URL = "https://os.unil.cloud.switch.ch/fma/fma_small.zip"
    FMA_ARCHIVE = FMA_DIR / "fma_small.zip"

    if not (FMA_DIR / "fma_small").exists():
        print("Downloading FMA Small...")
        subprocess.run(["wget", "-P", str(FMA_DIR), FMA_URL])
        print("Extracting...")
        subprocess.run(["unzip", "-q", str(FMA_ARCHIVE), "-d", str(FMA_DIR)])
        print("‚úì FMA Small ready")
    else:
        print("‚úì FMA Small already exists")

    fma_files = list((FMA_DIR / "fma_small").rglob('*.mp3'))
    print(f"Found {len(fma_files)} FMA audio files")
else:
    print("‚äò Skipping FMA download - will train on NESMDB only")
    fma_files = []

"""## 3. Prepare Training Dataset

"""

# Create PyTorch Dataset for training
import torchaudio
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import random

class MusicDataset(Dataset):
    """Dataset for MusicGen fine-tuning"""

    def __init__(self, audio_files, sample_rate=32000, duration=30, augment=True):
        self.audio_files = audio_files
        self.sample_rate = sample_rate
        self.duration = duration
        self.samples_per_file = int(duration * sample_rate)
        self.augment = augment

    def __len__(self):
        return len(self.audio_files)

    def __getitem__(self, idx):
        audio_path = self.audio_files[idx]

        try:
            # Load audio
            waveform, sr = torchaudio.load(audio_path)

            # Convert to mono
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)

            # Resample if needed
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                waveform = resampler(waveform)

            # Handle length
            if waveform.shape[1] > self.samples_per_file:
                # Random crop during training
                start = random.randint(0, waveform.shape[1] - self.samples_per_file)
                waveform = waveform[:, start:start + self.samples_per_file]
            elif waveform.shape[1] < self.samples_per_file:
                # Pad if too short
                padding = self.samples_per_file - waveform.shape[1]
                waveform = F.pad(waveform, (0, padding))

            # Simple augmentation (optional)
            if self.augment and random.random() > 0.5:
                # Random volume adjustment
                volume_factor = random.uniform(0.8, 1.2)
                waveform = waveform * volume_factor

            return {
                'audio': waveform.squeeze(0),  # Remove channel dimension
                'path': str(audio_path)
            }

        except Exception as e:
            print(f"Error loading {audio_path}: {e}")
            # Return silence if file fails to load
            return {
                'audio': torch.zeros(self.samples_per_file),
                'path': str(audio_path)
            }

# Check if required variables are defined (in case cells were skipped after runtime restart)
if 'DATA_DIR' not in globals():
    from pathlib import Path
    from google.colab import drive
    print("  DATA_DIR not found - re-initializing directories...")
    drive.mount('/content/drive', force_remount=False)
    PROJECT_DIR = Path('/content/drive/MyDrive/MusicGen_Training')
    DATA_DIR = PROJECT_DIR / 'datasets'
    OUTPUT_DIR = PROJECT_DIR / 'outputs'
    CHECKPOINT_DIR = PROJECT_DIR / 'checkpoints'
    LOGS_DIR = PROJECT_DIR / 'logs'
    print(f"‚úì Directories re-initialized")

if 'NESMDB_AUDIO_DIR' not in globals():
    NESMDB_AUDIO_DIR = DATA_DIR / 'nesmdb_audio'
    print(f"‚úì NESMDB_AUDIO_DIR set to: {NESMDB_AUDIO_DIR}")

# Collect all audio files
all_audio_files = list(NESMDB_AUDIO_DIR.glob('*.wav'))

# Add FMA files if they were downloaded
try:
    if 'fma_files' in globals() and fma_files:
        all_audio_files.extend(fma_files[:1000])  # Limit FMA to 1000 files for faster training
        print(f"Including {min(len(fma_files), 1000)} FMA files")
except:
    pass

print(f"Total training files: {len(all_audio_files)}")

# Check if we have enough files
if len(all_audio_files) < 10:
    print("\n‚ö†Ô∏è  WARNING: Very few training files found!")
    print("   Make sure MIDI files were converted to audio successfully.")
    print(f"   Audio directory: {NESMDB_AUDIO_DIR}")
    print(f"   Files found: {len(all_audio_files)}")

# Split into train/val
random.shuffle(all_audio_files)
split_idx = int(0.9 * len(all_audio_files))
train_files = all_audio_files[:split_idx]
val_files = all_audio_files[split_idx:]

print(f"Training samples: {len(train_files)}")
print(f"Validation samples: {len(val_files)}")

# Create datasets
train_dataset = MusicDataset(train_files, duration=30, augment=True)
val_dataset = MusicDataset(val_files, duration=30, augment=False)

print("\n‚úì Datasets created")

"""## 4. Load MusicGen Model for Fine-tuning

"""

# Load pre-trained MusicGen model
from audiocraft.models import MusicGen
from audiocraft.modules.conditioners import ConditioningAttributes

print("Loading MusicGen model...")
model = MusicGen.get_pretrained('facebook/musicgen-melody')

# Get device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"‚úì Using device: {device}")

# Get the language model and compression model components
lm = model.lm
compression_model = model.compression_model

# Move components to GPU (MusicGen wrapper doesn't have .to() method)
lm = lm.to(device)
compression_model = compression_model.to(device)

print(f"‚úì Model loaded on {device}")
print(f"‚úì Language model parameters: {sum(p.numel() for p in lm.parameters()) / 1e6:.1f}M")
print(f"‚úì Compression model parameters: {sum(p.numel() for p in compression_model.parameters()) / 1e6:.1f}M")
print(f"‚úì Language model ready for fine-tuning")

"""## 5. Training Configuration

"""

# Training hyperparameters
import torch.optim as optim

TRAINING_CONFIG = {
    'batch_size': 2,  # Small batch size due to memory constraints
    'learning_rate': 1e-5,
    'num_epochs': 5,
    'gradient_accumulation_steps': 8,  # Effective batch size = 2 * 8 = 16
    'warmup_steps': 100,
    'save_every': 500,  # Save checkpoint every N steps
    'eval_every': 250,  # Evaluate every N steps
    'max_grad_norm': 1.0,
    'weight_decay': 0.01,
}

print("Training Configuration:")
for key, value in TRAINING_CONFIG.items():
    print(f"  {key}: {value}")

# Create dataloaders
train_loader = DataLoader(
    train_dataset,
    batch_size=TRAINING_CONFIG['batch_size'],
    shuffle=True,
    num_workers=2,
    pin_memory=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=TRAINING_CONFIG['batch_size'],
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

print(f"\n‚úì Training batches per epoch: {len(train_loader)}")
print(f"‚úì Validation batches: {len(val_loader)}")

# Setup optimizer
optimizer = optim.AdamW(
    lm.parameters(),
    lr=TRAINING_CONFIG['learning_rate'],
    weight_decay=TRAINING_CONFIG['weight_decay']
)

# Learning rate scheduler with warmup
from torch.optim.lr_scheduler import LambdaLR

def lr_lambda(step):
    if step < TRAINING_CONFIG['warmup_steps']:
        return step / TRAINING_CONFIG['warmup_steps']
    return 1.0

scheduler = LambdaLR(optimizer, lr_lambda)

print("\n‚úì Optimizer and scheduler configured")

"""## 6. Training Loop

"""

# Training function
import time
from tqdm import tqdm
from IPython.display import clear_output

def train_epoch(model, train_loader, optimizer, scheduler, device, config, epoch):
    """Train for one epoch"""
    lm.train()
    compression_model.eval()  # Keep compression model frozen

    total_loss = 0
    optimizer.zero_grad()

    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}")):
        try:
            audio = batch['audio'].to(device)

            # Encode audio to tokens using compression model
            with torch.no_grad():
                codes, _ = compression_model.encode(audio.unsqueeze(1))  # Add channel dim

            # Simple training approach: predict next token
            # codes[0] shape: [batch, sequence_length]
            input_codes = codes[0][:, :-1]  # All but last token
            target_codes = codes[0][:, 1:]   # All but first token

            # Forward pass through language model
            # Note: This is a simplified approach - actual MusicGen training is more complex
            logits = lm.forward(input_codes)

            # Compute loss
            loss = F.cross_entropy(
                logits.reshape(-1, logits.size(-1)),
                target_codes.reshape(-1),
                ignore_index=-100
            )

            # Normalize loss for gradient accumulation
            loss = loss / config['gradient_accumulation_steps']
            loss.backward()

            # Gradient accumulation
            if (batch_idx + 1) % config['gradient_accumulation_steps'] == 0:
                # Clip gradients
                torch.nn.utils.clip_grad_norm_(lm.parameters(), config['max_grad_norm'])

                # Update weights
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()

            total_loss += loss.item() * config['gradient_accumulation_steps']

        except Exception as e:
            print(f"\nError in batch {batch_idx}: {e}")
            continue

        # Clear GPU cache periodically
        if batch_idx % 50 == 0:
            torch.cuda.empty_cache()

    return total_loss / len(train_loader)

def validate(model, val_loader, device):
    """Validate the model"""
    lm.eval()
    compression_model.eval()

    total_loss = 0

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validating"):
            try:
                audio = batch['audio'].to(device)

                # Encode audio
                codes, _ = compression_model.encode(audio.unsqueeze(1))

                # Forward pass
                input_codes = codes[0][:, :-1]
                target_codes = codes[0][:, 1:]
                logits = lm.forward(input_codes)

                # Compute loss
                loss = F.cross_entropy(
                    logits.reshape(-1, logits.size(-1)),
                    target_codes.reshape(-1),
                    ignore_index=-100
                )

                total_loss += loss.item()
            except Exception as e:
                print(f"\nError in validation batch: {e}")
                continue

    return total_loss / len(val_loader) if len(val_loader) > 0 else 0

print("‚úì Training functions defined")

# Start training!
print("="*60)
print("STARTING TRAINING")
print("="*60)
print(f"Training for {TRAINING_CONFIG['num_epochs']} epochs")
print(f"Total training samples: {len(train_dataset)}")
print(f"Checkpoints will be saved to: {CHECKPOINT_DIR}")
print("="*60)

training_history = {
    'train_loss': [],
    'val_loss': [],
    'epochs': []
}

best_val_loss = float('inf')
start_time = time.time()

for epoch in range(TRAINING_CONFIG['num_epochs']):
    print(f"\n{'='*60}")
    print(f"EPOCH {epoch + 1}/{TRAINING_CONFIG['num_epochs']}")
    print(f"{'='*60}")

    # Train
    train_loss = train_epoch(
        model, train_loader, optimizer, scheduler,
        device, TRAINING_CONFIG, epoch
    )

    # Validate
    val_loss = validate(model, val_loader, device)

    # Record history
    training_history['train_loss'].append(train_loss)
    training_history['val_loss'].append(val_loss)
    training_history['epochs'].append(epoch + 1)

    # Print stats
    elapsed = time.time() - start_time
    print(f"\nEpoch {epoch + 1} Summary:")
    print(f"  Train Loss: {train_loss:.4f}")
    print(f"  Val Loss: {val_loss:.4f}")
    print(f"  Time: {elapsed/60:.1f} minutes")
    print(f"  LR: {scheduler.get_last_lr()[0]:.2e}")

    # Save checkpoint
    checkpoint_path = CHECKPOINT_DIR / f"checkpoint_epoch_{epoch+1}.pt"
    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': lm.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'config': TRAINING_CONFIG,
    }, checkpoint_path)
    print(f"  ‚úì Checkpoint saved: {checkpoint_path.name}")

    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_path = CHECKPOINT_DIR / "best_model_wii.pt"
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': lm.state_dict(),
            'val_loss': val_loss,
            'config': TRAINING_CONFIG,
        }, best_model_path)
        print(f"  ‚úì New best model saved! (val_loss: {val_loss:.4f})")

total_time = time.time() - start_time
print(f"\n{'='*60}")
print("TRAINING COMPLETE!")
print(f"{'='*60}")
print(f"Total time: {total_time/3600:.2f} hours")
print(f"Best validation loss: {best_val_loss:.4f}")
print(f"Final model saved to: {CHECKPOINT_DIR}")

# Save training history
history_path = LOGS_DIR / 'training_history.json'
with open(history_path, 'w') as f:
    json.dump(training_history, f, indent=2)
print(f"Training history saved to: {history_path}")

# Clean up old checkpoints to save space
import os
from pathlib import Path

# Find all checkpoint files
checkpoints = sorted(CHECKPOINT_DIR.glob('checkpoint_epoch_*.pt'))

if len(checkpoints) == 0:
    print("‚ö†Ô∏è  No checkpoints found.")
else:
    print(f"Found {len(checkpoints)} checkpoint(s)")
    print("\nCurrent checkpoints:")
    total_size = 0
    for cp in checkpoints:
        size_mb = os.path.getsize(cp) / (1024 * 1024)
        total_size += size_mb
        print(f"  - {cp.name} ({size_mb:.1f} MB)")

    print(f"\nTotal size: {total_size:.1f} MB")

    # Check if best_model.pt exists
    best_model_path = CHECKPOINT_DIR / "best_model.pt"
    if best_model_path.exists():
        best_size_mb = os.path.getsize(best_model_path) / (1024 * 1024)
        print(f"  - best_model.pt ({best_size_mb:.1f} MB) [WILL KEEP]")
        total_size += best_size_mb

    if len(checkpoints) <= 1:
        print("\n‚úì Only one checkpoint exists. Nothing to clean up!")
    else:
        print(f"\n{'='*60}")
        print("CLEANUP PLAN")
        print(f"{'='*60}")

        # Keep the most recent checkpoint
        latest_checkpoint = checkpoints[-1]
        checkpoints_to_delete = checkpoints[:-1]

        print(f"KEEP: {latest_checkpoint.name} (most recent)")
        if best_model_path.exists():
            print(f"KEEP: best_model.pt (best validation loss)")

        print(f"\nDELETE ({len(checkpoints_to_delete)} files):")
        space_to_free = 0
        for cp in checkpoints_to_delete:
            size_mb = os.path.getsize(cp) / (1024 * 1024)
            space_to_free += size_mb
            print(f"  - {cp.name} ({size_mb:.1f} MB)")

        print(f"\nüíæ Space to free: {space_to_free:.1f} MB")

        # Ask for confirmation
        confirm = input(f"\nDelete {len(checkpoints_to_delete)} old checkpoint(s)? (yes/no): ").strip().lower()

        if confirm == 'yes':
            print("\nDeleting old checkpoints...")
            deleted_count = 0
            for cp in checkpoints_to_delete:
                try:
                    os.remove(cp)
                    print(f"  ‚úì Deleted: {cp.name}")
                    deleted_count += 1
                except Exception as e:
                    print(f"  ‚úó Failed to delete {cp.name}: {e}")

            print(f"\n{'='*60}")
            print(f"‚úì Cleanup complete!")
            print(f"{'='*60}")
            print(f"Deleted: {deleted_count} file(s)")
            print(f"Freed: {space_to_free:.1f} MB")
            print(f"\nRemaining checkpoints:")
            print(f"  - {latest_checkpoint.name}")
            if best_model_path.exists():
                print(f"  - best_model.pt")
        else:
            print("\n‚ùå Cleanup cancelled.")

# Load best checkpoint
from audiocraft.data.audio import audio_write
from IPython.display import Audio, display

print("Loading best fine-tuned model...")
best_checkpoint = torch.load(CHECKPOINT_DIR / "best_model_wii.pt", map_location=device)
lm.load_state_dict(best_checkpoint['model_state_dict'])

# Set models to eval mode (only call on actual model components, not wrapper)
lm.eval()
compression_model.eval()

print(f"‚úì Loaded model from epoch {best_checkpoint['epoch']}")
print(f"‚úì Validation loss: {best_checkpoint['val_loss']:.4f}")

# Set generation parameters
model.set_generation_params(
    duration=15,  # Generate 15 seconds
    temperature=1.0,
    top_k=250,
    cfg_coef=3.0
)

print("\n‚úì Model ready for generation!")

# Resume training from checkpoint
import os

# Check for existing checkpoints
checkpoints = sorted(CHECKPOINT_DIR.glob('checkpoint_epoch_*.pt'))

if len(checkpoints) == 0:
    print("‚ö†Ô∏è  No checkpoints found. Please run Cell 17 to start training from scratch.")
else:
    print("Available checkpoints:")
    for i, cp in enumerate(checkpoints):
        print(f"  {i+1}. {cp.name}")

    # Load the latest checkpoint
    latest_checkpoint = checkpoints[-1]
    print(f"\nLoading checkpoint: {latest_checkpoint.name}")

    checkpoint = torch.load(latest_checkpoint, map_location=device)

    # Restore model, optimizer, and scheduler state
    lm.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    start_epoch = checkpoint['epoch']
    print(f"‚úì Resuming from epoch {start_epoch}")
    print(f"  Previous train loss: {checkpoint['train_loss']:.4f}")
    print(f"  Previous val loss: {checkpoint['val_loss']:.4f}")

    # Set number of additional epochs to train
    additional_epochs = int(input("\nHow many additional epochs to train? "))
    total_epochs = start_epoch + additional_epochs

    print(f"\n{'='*60}")
    print("RESUMING TRAINING")
    print(f"{'='*60}")
    print(f"Starting from epoch: {start_epoch}")
    print(f"Training until epoch: {total_epochs}")
    print(f"Additional epochs: {additional_epochs}")
    print(f"{'='*60}")

    # Load existing training history if available
    history_path = LOGS_DIR / 'training_history.json'
    if history_path.exists():
        with open(history_path, 'r') as f:
            training_history = json.load(f)
        print(f"‚úì Loaded existing training history")
    else:
        training_history = {
            'train_loss': [],
            'val_loss': [],
            'epochs': []
        }

    best_val_loss = min(training_history['val_loss']) if training_history['val_loss'] else float('inf')
    start_time = time.time()

    # Continue training
    for epoch in range(start_epoch, total_epochs):
        print(f"\n{'='*60}")
        print(f"EPOCH {epoch + 1}/{total_epochs}")
        print(f"{'='*60}")

        # Train
        train_loss = train_epoch(
            model, train_loader, optimizer, scheduler,
            device, TRAINING_CONFIG, epoch
        )

        # Validate
        val_loss = validate(model, val_loader, device)

        # Record history
        training_history['train_loss'].append(train_loss)
        training_history['val_loss'].append(val_loss)
        training_history['epochs'].append(epoch + 1)

        # Print stats
        elapsed = time.time() - start_time
        print(f"\nEpoch {epoch + 1} Summary:")
        print(f"  Train Loss: {train_loss:.4f}")
        print(f"  Val Loss: {val_loss:.4f}")
        print(f"  Time: {elapsed/60:.1f} minutes")
        print(f"  LR: {scheduler.get_last_lr()[0]:.2e}")

        # Save checkpoint
        checkpoint_path = CHECKPOINT_DIR / f"checkpoint_epoch_{epoch+1}.pt"
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': lm.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'config': TRAINING_CONFIG,
        }, checkpoint_path)
        print(f"  ‚úì Checkpoint saved: {checkpoint_path.name}")

        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_path = CHECKPOINT_DIR / "best_model_wii.pt"
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': lm.state_dict(),
                'val_loss': val_loss,
                'config': TRAINING_CONFIG,
            }, best_model_path)
            print(f"  ‚úì New best model saved! (val_loss: {val_loss:.4f})")

    total_time = time.time() - start_time
    print(f"\n{'='*60}")
    print("TRAINING COMPLETE!")
    print(f"{'='*60}")
    print(f"Total additional time: {total_time/3600:.2f} hours")
    print(f"Total epochs trained: {total_epochs}")
    print(f"Best validation loss: {best_val_loss:.4f}")

    # Save updated training history
    with open(history_path, 'w') as f:
        json.dump(training_history, f, indent=2)
    print(f"Training history saved to: {history_path}")

"""## 7. Generate Music with Fine-tuned Model

"""

# Load best checkpoint
from audiocraft.data.audio import audio_write
from IPython.display import Audio, display

print("Loading best fine-tuned model...")
best_checkpoint = torch.load(CHECKPOINT_DIR / "best_model_wii.pt", map_location=device)
lm.load_state_dict(best_checkpoint['model_state_dict'])

# Set models to eval mode (only call on actual model components, not wrapper)
lm.eval()
compression_model.eval()

print(f"‚úì Loaded model from epoch {best_checkpoint['epoch']}")
print(f"‚úì Validation loss: {best_checkpoint['val_loss']:.4f}")

# Set generation parameters
model.set_generation_params(
    duration=15,  # Generate 15 seconds
    temperature=1.0,
    top_k=250,
    cfg_coef=3.0
)

print("\n‚úì Model ready for generation!")

# Generate music with fine-tuned model
test_prompts = [
    "slow calm town theme"
]

output_dir = OUTPUT_DIR / 'finetuned_samples'

print("Generating music with fine-tuned model...")
print(f"Output directory: {output_dir}\n")

for i, prompt in enumerate(test_prompts):
    print(f"\n[{i+1}/{len(test_prompts)}] Prompt: {prompt}")

    # Generate
    wav = model.generate([prompt], progress=True)

    # Save
    output_path = output_dir / f"finetuned_{i:02d}"
    audio_write(
        str(output_path),
        wav[0].cpu(),
        model.sample_rate,
        strategy="loudness",
        loudness_compressor=True
    )

    print(f"‚úì Saved: {output_path}.wav")

    # Play in notebook
    display(Audio(wav[0].cpu().numpy(), rate=model.sample_rate))

print(f"\n{'='*60}")
print("GENERATION COMPLETE!")
print(f"{'='*60}")
print(f"Generated {len(test_prompts)} samples")
print(f"Saved to: {output_dir}")

"""## 8. Compare with Pre-trained Model (Optional)

"""

# Generate with original pre-trained model for comparison
print("Loading original pre-trained model for comparison...")
pretrained_model = MusicGen.get_pretrained('facebook/musicgen-melody')
pretrained_model.set_generation_params(duration=15, temperature=1.0, cfg_coef=3.0)

comparison_prompt = "8-bit chiptune video game music"

print(f"\nGenerating with prompt: {comparison_prompt}")
print("\n[1/2] Pre-trained model...")
pretrained_wav = pretrained_model.generate([comparison_prompt], progress=True)

print("\n[2/2] Fine-tuned model...")
finetuned_wav = model.generate([comparison_prompt], progress=True)

# Save both
pretrained_path = output_dir / "comparison_pretrained"
finetuned_path = output_dir / "comparison_finetuned"

audio_write(str(pretrained_path), pretrained_wav[0].cpu(), pretrained_model.sample_rate)
audio_write(str(finetuned_path), finetuned_wav[0].cpu(), model.sample_rate)

print("\n" + "="*60)
print("COMPARISON")
print("="*60)
print("\nPre-trained model:")
display(Audio(pretrained_wav[0].cpu().numpy(), rate=pretrained_model.sample_rate))

print("\nFine-tuned model:")
display(Audio(finetuned_wav[0].cpu().numpy(), rate=model.sample_rate))

print(f"\n‚úì Comparison samples saved to: {output_dir}")

## Summary

print("\n" + "="*60)
print("PROJECT SUMMARY")
print("="*60)
print(f"\nProject directory: {PROJECT_DIR}")
print(f"\nTraining:")
print(f"  - Dataset: Wii ({len(train_dataset)} samples)")
print(f"  - Epochs: {TRAINING_CONFIG['num_epochs']}")
print(f"  - Best val loss: {best_val_loss:.4f}")
print(f"  - Checkpoints: {CHECKPOINT_DIR}")
print(f"\nGenerated samples: {len(list(output_dir.glob('*.wav')))}")
print(f"Output directory: {output_dir}")
print(f"\nNext steps:")
print(f"  - Listen to generated samples")
print(f"  - Compare fine-tuned vs pre-trained")
print(f"  - Experiment with different prompts")
print(f"  - Try longer training for better results")
print("="*60)